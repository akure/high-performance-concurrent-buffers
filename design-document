This is the design document for the high performance concurrent buffer project.

In this project I am planning to implement buffers based on different data structures which can server as concurrent buffers 
for multi-threaded system design. 

The programming language that we concentrate is C++0x ( C++11 ) 

We would also compare the performace of different buffers in different scenarios, initially we would consider the 
situation where the server is receiving a very high load of input data lets sat 15,000 msg per sec continously for
n ( say 8 hours a day in fix time) Well the input rate pattern can vary between 10,000 msg per sec to 15,000 msg per sec.
We have to design a concurrent buffer that can be used in such a situation, It should be scalable and enough flexible for 
multiple threads. It should be able to support multi producer multi consumer patterns for high processing.

There should be no cumulative data accumulation in the buffers for a long time performance. 
Data should be consumable from the buffers.
Also there should be no loss in the input data which are coming from the network. 

List of concurrent buffers:
1. concurrent dequeue 

2. concurrent vector

3. concurrent map

4. concurrent  linked list

5. concurrent nested buffers
    we will develop this concept as we go ahead with basic concurrent buffers.
    
Concurrent deque design:

producer threads   deque      consumer threads

write mutex  [one common write buffer]  read mutex  
  
  ----                                ----  local buffer ----
  ----          ==============        ----  local buffer ----  
  ----                                ----  local buffer ----
  
In the above situation where there is a one common write buffer where producers are keep on writting their data. 
How do consumer threads come to know that they have to pick up something from the buffer and remember we have multiple 
consumer.

What are different strategy we can design form consumers?
Consumer thread picking strategy:
   All consumer thread will run independently.
   We can have one load distributer thread, just before consumers. In this case consumer threads will just look up 
   into their local buffer. 
   Call for a thread to one of the free consumer threads to pick up the data once the data size becomes of 
   some defined size n. 
   
Asycn strategy:
   consumer threads to use an async buffer read timer, wherein after timer expiry it will copy n elements 
   to its local buffer and then process the data. ofcourse it will check if there are n element available 
   in the buffer. If less then n elememnt exist, it will copy all the element. Well here I am saying copy initially.
   but we will implement move version of the same concpet for high speed performance.
   Its a strategy where an asyn function in a thread wakes up on his own and do its work and then sleep again. 
   
Pickup call strategy:
 Consumer thread will be informed by the producer threads or by some other monitoring entity in the system.
 Once a condition is reached like enough size is available in the buffer, then consumer will be informed.
 
Load distributer strategy:
 Here in this case there is one load distributet thread just before the consumers. It can itself be async or pickup call
 strategy. but once it knows there is data  available in the buffer in its active situation, it can again can have
 push/move strategy or pickup call strategy or assignment strategy. 
 
 Push or move strategy of load distributer thread:
 
 Pick up call strategy by load distributer thread:
 
 Assignment strategy by load distributer thread:
 
 
Async timer Vs Signal events between threads, which one should be used?
    








